\chapter{Introduction}

Elementary particle physics aims to describe nature at its most fundamental level.
This thesis presents analyses of data obtained from high energy proton collisions, which search for evidence for or against new theoretical models of elementary particle physics.
This section introduces the theoretical context of these searches, describing first the theoretical model presently serving as the null hypothesis of elementary particle physics, then a few issues identified with this model, and finally some proposed solutions for these issues.

\section{The Standard Model of Particle Physics} \label{sec:standardmodel}

Modern elementary particle physics finds itself in a peculiar state.
Physicists have constructed a theoretical model, now called the Standard Model of Particle Physics (the Standard Model; SM), that is the most quantitatively accurate scientific model of any kind, consistent with experiment in almost every laboratory test, even when experimental uncertainty is smaller than a part per ten billion \cite{electronmu_exp,electronmu_th}, with its most famously accurate prediction displayed in Table~\ref{tab:electronmu}.
\begin{table}
\centering
\begin{tabular}{r l}
\multicolumn{2}{c}{$\mu_e$} \\
\hline
Experiment & 2.00231930436 (56) \\
Theory & 2.002319304363 (15)
\end{tabular}
\caption[Comparison of theoretical prediction and experimental measurement of the electron magnetic moment.]
        {A comparison of the leading experimental measurement of the electron's magnetic moment \cite{electronmu_exp}, and the Standard Model's theoretical prediction \cite{electronmu_th}, in terms of the classical prediction, the Bohr magneton. 
          Famously, this is the most accurate verification of theory by experiment in all of science. 
          The numbers in parentheses are the uncertainties, on the same order as the last digits printed.}
\label{tab:electronmu}
\end{table}

It is important to take this time to reflect on the Standard Model's astonishing accuracy, because the Standard Model is known with certainty to be imperfect, for a few reasons. 
A subset are discussed in Section~\ref{sec:SMproblems}.
As a result, the Standard Model is sometimes treated with disdain, belying its quantitative success as a model of nature.
This attitude is born from frustration.
Particle physicists have a theory {\it known} to be incomplete, only an approximation, and yet so accurate an approximation that no evidence in favor of any specific proposed extension has ever been found.
Modern particle physicists hope to find experimental clues by testing the Standard Model using any available means, including, as in this work, by comparing its predictions for the outcomes of high energy proton collisions with experimental data.

  \subsection{A Brief Description} \label{sec:SMdescription}

  The Standard Model is based in quantum field theory (QFT), which provides the general toolkit used to make calculations, and identifies the observed elementary particles as excitations of these underlying fields.
  Since every elementary particle corresponds directly to an underlying field and vice-versa, for example ``the electron particle'' and ``the electron field'' are colloquially treated as synonyms.

  The SM itself, like every model of elementary particle physics based in QFT, consists of a list of particles present in nature and a description of their interactions with each other.
  The particles can be grouped in several ways, but by far the most significant is to group bosons and fermions.

  The bosons of the Standard Model are the photon (typically indicated by $\gamma$), the W$^+$ and W$^-$, the Z, the eight gluons, and the Higgs.
  Of these, the Higgs has no intrinsic angular momentum (``spin-0'') and the others have spin quantum number 1.
  One of the major insights obtained from QFT is that the existence of spin-1 particles requires the existence of fermions that interact with them, and vice versa.
  A fermion that interacts with a boson is said to be ``charged under'' the boson.
  Moreover, pure fermion interactions are forbidden, so that fermion-fermion interactions require a ``mediating'' boson under which both fermions are charged, while boson-boson interactions are perfectly acceptable.
  For this reason, bosons are said to mediate fundamental interactions.
  The photon mediates electromagnetism, and the gluons collectively mediate the strong interaction, as described by Quantum Chromodynamics (QCD).
  For historical reasons, the distinct interactions mediated by the W$^+$ and W$^-$, the Z, and the Higgs are all collectively called the weak interaction.
  The bosons of the Standard Model are summarized in Table~\ref{tab:bosons}.

  \begin{table}
    \centering
    \begin{tabular}{r r l}
      Boson             & Mass (GeV) \cite{pdg} & Interaction \\
      \hline
      Photon ($\gamma$) & 0                     & Electromagnetism \\
      Gluon (g)         & 0                     & Strong (QCD) \\
      W$^{\pm}$         & 80.379 $\pm$ 0.012    & Weak (charged current) \\
      Z                 & 91.1876 $\pm$ 0.0021  & Weak (neutral current) \\
      Higgs (h)         & 125.18 $\pm$ 0.16     & Weak (symmetry breaking and mass) \\
    \end{tabular}
    \caption[Table of Standard Model bosons.]
            {The Standard Model bosons, their masses, and associated interactions. Note that there are eight gluons, but they are not experimentally distinct.}
            \label{tab:bosons}
  \end{table}

  The interactions have very different character due to differences in the mediating bosons.
  The photon and electromagnetism are most familiar.
  Electromagnetism is relatively simple, as a result of having only a single-component charge and a single mediating boson, and is relatively easily studied due to its long range, as a consequence of the masslessness of the photon.
  The weak interaction, by contrast, is less well-known in part due to its very short range, a consequence of the large masses of the mediating bosons.
  In addition to the $1/r^2$ behavior familiar from electromagnetism, forces are also suppressed by an exponential term, $e^{-\frac{mc^2}{\hbar c}r}$, where $m$ is the mass of the mediating boson and $\,\hbar c \approx 200$~GeV-pm.
  This term vanishes for massless bosons like the photon, but since the weak bosons have masses on the order of 100~GeV, the weak interaction becomes negligible after only a few picometers despite having an intrinsic interaction strength comparable to electromagnetism.
  Thus, the weak interaction has an impact only on nuclear and elementary particle physics.
  The strong interaction is also short ranged, with effects becoming significant only on the order of femtometers, but for an entirely different reason.
  As its name suggests, it is also intrinsically stronger than the other interactions.
  It will be discussed in Section~\ref{sec:hadronization}.

  The fermions of the Standard Model are all spin-$\frac{1}{2}$, and come in two major groups.
  The leptons are those fermions which do not participate in the strong interaction, and the quarks are those that do.
  The leptons can be further subdivided into the electrically charged leptons, like the electron, and the electrically neutral neutrinos, which having neither strong nor electromagnetic interactions can interact only weakly, and are ghost-like particles as a result.
  The quarks can be further subdivided into the up-type quarks, which are positively charged, and the down-type quarks, which are negatively charged.
  There are three of each quark, one for each of the charges of the strong interaction, but like the eight gluons, they are not experimentally distinct.

  As is evident in Table~\ref{tab:fermions}, the fermions of the Standard Model occur in triplets with identical properties aside from mass: the up-type quarks, the down-type quarks, the charged leptons, and the neutrinos are all roughly three versions of the same particle with different masses.
  The fermions are therefore said to be arranged in three similar generations, with generation 1 having the least mass and generation 3 the greatest.

  \renewcommand{\arraystretch}{1.2}
  \begin{table}
    \centering
    \begin{tabular}{l c r r}
      Fermion             & Symbol & Mass (GeV) \cite{pdg} & Electric charge \\
      \hline
      Up quark            & $u$    & $0.0022^{+0.0005}_{-0.0004}$      & +$\frac{2}{3}$  \\
      Charm quark         & $c$    & $1.275^{+0.025}_{-0.035}$       & +$\frac{2}{3}$  \\
      Top quark           & $t$    & $173.0 \pm 0.4$       & +$\frac{2}{3}$  \\
      Down quark$^*$      & $d$    & $0.0047^{+0.0005}_{-0.0003}$      & -$\frac{1}{3}$  \\
      Strange quark       & $s$    & $0.095^{+0.009}_{-0.003}$       & -$\frac{1}{3}$  \\
      Bottom quark        & $b$    & $4.18^{+0.04}_{-0.03}$       & -$\frac{1}{3}$  \\
      \hline
      Electron            & $e$    & $0.0005109989461$(31) & $-1$ \\
      Muon                & $\mu$    & $0.1056583745$(24)  & $-1$ \\
      Tau                 & $\tau$    & $1.77686$(12)  & $-1$ \\
      Neutrinos           & $\nu_{e}$, $\nu_{\mu}$, $\nu_{\tau}$ & 0$^{\dagger}$ & 0 \\
    \end{tabular}
    \caption[Table of Standard Model fermions.]
    {A table of the Standard Model fermions, first the quarks then the leptons.
      The vastly different experimental character of quarks due to their confinement by the strong interactions make their masses much more difficult to measure than those of the charged leptons.
      Note that every Standard Model fermion also has an antiparticle, with identical properties except for opposite charge.
      (*) The charge and mass eigenstates of (by convention) down-type quarks are different, so that for example ``the mass of the down quark'' does not exist. Otherwise, decays across generations would be impossible. In practice, they are so nearly equal that the mass of the down quark is equated with that of the lightest mass eigenstate, etc.
      ($\dagger$) The Standard Model predicts that neutrinos are massless but they experimentally have nonzero masses, albeit one million times smaller than the electron's (see Section~\ref{sec:SMproblems}).
}
            \label{tab:fermions}
  \end{table}
  \renewcommand{\arraystretch}{1}

  If each generation were truly independent of the others, then each would be independently stable, as there would be no way to cross from, say, generation 3 to generation 1.
  However, it is an experimental fact that quarks can decay across generations via their interactions with the W boson, albeit much more slowly than within the same generation.
  This is well-accommodated in the Standard Model \cite{cabibbo,ckm}, in which there is no reason in general for the charge eigenstates with respect to any given boson to be equal to those of any other.
  Famously, the Higgs boson is responsible for the masses of the Standard Model's elementary particles, with each particle acquiring a mass proportional to the strength of its interaction with the Higgs.
  The Higgs interaction eigenstates, being the mass eigenstates, are privileged as the only charge eigenstates that are also eigenstates of the Hamiltonian, and so are the only states that can be more than transient, the states of so-called ``real'' particles.
  The hybrid nature of the real states with respect to W interactions produces effective generation-crossing interactions for quarks described by the Cabibbo-Kobayashi-Maskawa (CKM) matrix, with values measured experimentally to be approximately \cite{pdg},
  \begin{equation} \label{eqn:ckm}
    \begin{bmatrix} 
      |V_{ud}| & |V_{us}| & |V_{ub}| \\
      |V_{cd}| & |V_{cs}| & |V_{cb}| \\
      |V_{td}| & |V_{ts}| & |V_{tb}| 
    \end{bmatrix}
\approx
    \begin{bmatrix} 
      0.974 & 0.224 & 0.004 \\
      0.218 & 0.997 & 0.042 \\
      0.008 & 0.039 & 1.019
    \end{bmatrix}
  \end{equation}
  where $V_{xy}$ indicates the effective coupling between quarks $x$ and $y$.
  If the eigenstates were exactly equal, the CKM matrix would be diagonal, with $V_{xx} = 1$ and $V_{xy} = 0$ ($x \neq y$), prohibiting trans-generation decays.
  As can be seen in Equation~\ref{eqn:ckm}, the CKM matrix is {\it nearly} diagonal, meaning trans-generation decays are slow but not impossible.
  This causes the third generation bottom quark to have a longer lifetime than the second generation charm quark despite having a larger mass, because a bottom quark must cross a generation to decay  (the top quark is more massive) using $|V_{ub}| \approx 0.004$ or $|V_{cb}| \approx 0.042$, while a charm quark does not (the strange quark, also of the second generation, is less massive) and may decay using $|V_{cs}| \approx 0.997$.
  In fact, the bottom quark lifetime is long enough that bottom hadron decay lengths are macroscopic, on the order of millimeters.
  This is experimentally relevant, as discussed in Section~\ref{sec:btagging}.
  Similar physics is possible for leptons since neutrinos are now known to be massive, but is not incorporated in the Standard Model, in which neutrinos are massless (see Section~\ref{sec:SMproblems}).

  Further description of the Standard Model's interactions is more conveniently done using Feynman Diagrams, in the next section.

  \subsection{Feynman Diagrams and Perturbative Expansions} \label{sec:feyndiags}

  The interactions of the Standard Model are expressed as a Lagrangian density of intimidating complexity.
  Applying the Euler-Lagrange equations to this Lagrangian produces nonlinear differential equations that have no known exact solutions.
  However, it is possible to extract an approximate solution as a perturbative expansion in powers of the coupling, a parameter that indicates the intrinsic strength of an interaction.
  This procedure at first seems barely feasible, as it is no trivial task to find all the contributions to the leading order term, then the next to leading order term, and so on by simple inspection of the Lagrangian.
  Fortunately, physicist Richard Feynman was able to devise a diagrammatic method for expressing the terms of the perturbative expansion that is far more intuitive.

  First, one may inspect each term of the Lagrangian to assemble so-called vertices, the building blocks of diagrams.
  Each field that occurs in a term represents one line, and all the lines of a term intersect at a central point to form the vertex.
  For example, the Standard Model Lagrangian contains a term in which the electron field appears twice and the photon field appears once, from which one obtains the vertex shown in Figure~\ref{fig:eegamma}.
  Each vertex represents one power of the coupling.
  The diagrams with the fewest vertices, then, are the lowest order diagrams in the perturbative expansion.

  \begin{figure}[h!]
    \centering
    \begin{fmffile}{eegamma}
      \begin{fmfgraph*}(40,25)
        \fmfleft{i1,i2}
        \fmfright{o1,o2}
        \fmfbottom{b}
        \fmf{fermion,label=$e^-$,label.side=left}{i2,v1}
        \fmf{fermion,label=$e^-$,label.side=left}{v1,o2}
        \fmf{photon,label=$\gamma$,label.side=left}{v1,b}
      \end{fmfgraph*}
    \end{fmffile}

    \caption[The fundamental vertex of electromagnetism.]{
      The Standard Model Lagrangian contains a term in which the electron field appears twice, and the photon field once, which corresponds to this Feynman diagram vertex.
      This vertex, and analogous vertices in which another electrically charged fermion replaces the electron, are the vertices of electromagnetism.      
      Traditionally, straight lines with arrows represent fermions, and a wavy line represents a photon (or a W or Z).
      The arrows on fermion lines mark whether a fermion is matter or antimatter, depending on whether the arrow points generally in the same (matter) or opposite (antimatter) direction as the flow of time (left to right).
    }
    \label{fig:eegamma}
  \end{figure}  

  Typically, time runs left to right in Feynman diagrams.
  The vertex in Figure~\ref{fig:eegamma}, then, depicts an incoming electron absorbing or emitting a photon, and continuing along.
  All Feynman vertices can be freely rotated, effectively changing the flow of time.
  For example, one can rotate Figure~\ref{fig:eegamma} to produce Figure~\ref{fig:eeannihilation}.
  Figure~\ref{fig:eeannihilation} depicts the annihilation of an electron and its antiparticle, the positron, into a photon.
  While this process is allowed by the Standard Model as shown, it is not allowed kinematically, as it is impossible to conserve energy and momentum with only a single massless particle in the final state.
  To produce the leading order diagram for electron-positron annihilation that is allowed by kinematics, two vertices must be {\it connected} as shown in Figure~\ref{fig:eeannihilation_allowed}, by attaching two identical external lines to form an internal line.

  \begin{figure}[h!]
    \centering
    \begin{fmffile}{eeannihilation}
      \begin{fmfgraph*}(40,25)
        \fmfleft{i1,i2}
        \fmfright{o1}
        \fmf{fermion,label=$e^-$,label.side=left}{i2,v1}
        \fmf{fermion,label=$e^+$,label.side=left}{v1,i1}
        \fmf{photon,label=$\gamma$,label.side=left}{v1,o1}
      \end{fmfgraph*}
    \end{fmffile}

    \caption[A rotation of the fundamental vertex of electromagnetism.]{
      This diagram is a rotation of Figure~\ref{fig:eegamma}.
      Instead of an incoming electron absorbing or emitting a photon, this diagram represents an electron and its antiparticle, the positron, annihilating into a photon.
      While permitted as an interaction by the Standard Model, this process is not kinematically allowed, making Figure~\ref{fig:eeannihilation_allowed} the leading order diagram for electron-positron annihilation.
    }
    \label{fig:eeannihilation}
  \end{figure}  

  \begin{figure}[h!]
    \centering
    \begin{fmffile}{eeannihilation_allowed}
      \begin{fmfgraph*}(40,25)
        \fmfleft{i1,i2}
        \fmfright{o1,o2}
        \fmf{fermion,label=$e^-$,label.side=left}{i2,v2}
        \fmf{fermion,label=$e$,label.side=left}{v2,v1}
        \fmf{fermion,label=$e^+$,label.side=left}{v1,i1}
        \fmf{photon,label=$\gamma$,label.side=left}{v1,o1}
        \fmf{photon,label=$\gamma$,label.side=left}{v2,o2}
      \end{fmfgraph*}
    \end{fmffile}

    \caption[The leading order diagram of electron-positron annihilation.]{
      This diagram connects vertices like those of Figures~\ref{fig:eegamma}~and~\ref{fig:eeannihilation} to produce the leading order kinematically allowed diagram for electron and positron annihilation.
      Whether the internal line is an electron or positron is ambiguous, due to the relativity of simultaneity.
      In some reference frames, the positron emits a photon first, then annihilates with the electron, and so the internal line is a positron.
      In others, the electron emits a photon first, then annihilates with the positron, and the internal line is an electron.
    }
    \label{fig:eeannihilation_allowed}
  \end{figure}  

  All of the diagrams shown thus far have been ``tree-level,'' with no internal loops.
  These tend to be the leading order diagrams, but diagrams with internal loops also contribute to the amplitude, usually at sub-leading order.
  Figure~\ref{fig:eeannihilation_loop} is one such diagram, contributing to the next-to-leading order term of the electron-positron annihilation amplitude.

  \begin{figure}[h!]
    \centering
    \begin{fmffile}{eeannihilation_loop}
      \begin{fmfgraph*}(40,25)
        \fmfleft{i1,i2}
        \fmfright{o1,o2}
        \fmf{fermion}{i2,vi}
        \fmf{fermion}{vi,v2}
        \fmf{photon,right=1}{vi,vf}
        \fmf{fermion}{v2,vf}
        \fmf{fermion}{vf,v1}
        \fmf{fermion}{v1,i1}
        \fmf{photon}{v1,o1}
        \fmf{photon}{v2,o2}
      \end{fmfgraph*}
    \end{fmffile}

    \caption[One of the next-to-leading order diagrams of electron-positron annihilation, containing an internal loop.]{
      This diagram is one of the next-to-leading order contributions to the electron-positron annihilation amplitude, containing an internal loop.
    }
    \label{fig:eeannihilation_loop}
  \end{figure}  

  In all of the diagrams we have reviewed thus far, the electron could be replaced with any electrically charged fermion, such as a muon or up quark.
  Collectively, all of these diagrams and others like them, constructed by combining variants of Figure~\ref{fig:eegamma}, constitute electromagnetism.

  The strong interaction as described by Quantum Chromodynamics (QCD) has a similar foundational diagram, shown in Figure~\ref{fig:qcdvertices} (upper), in which the photon is switched out for a gluon, and the fermion must be a quark.
  Unlike electromagnetism, the strong interaction also contains interactions between the mediating bosons, the gluons, alone.
  These vertices are partly responsible for the dramatically different physics of the strong interaction compared to electromagnetism.
  See Section~\ref{sec:hadronization} for more details.

  \begin{figure}[h!]
    \centering
    \begin{fmffile}{qqgluon}
      \begin{fmfgraph*}(40,25)
        \fmfleft{i1,i2}
        \fmfright{o1,o2}
        \fmfbottom{b}
        \fmf{fermion,label=$q$,label.side=left}{i2,v1}
        \fmf{fermion,label=$q$,label.side=left}{v1,o2}
        \fmf{gluon,label=$g$,label.side=left}{v1,b}
      \end{fmfgraph*}
    \end{fmffile}

    \begin{fmffile}{g3}
      \begin{fmfgraph*}(40,25)
        \fmfleft{i1,i2}
        \fmfright{o1,o2}
        \fmfbottom{b}
        \fmf{gluon,label=$g$,label.side=left}{i2,v1}
        \fmf{gluon,label=$g$,label.side=left}{v1,o2}
        \fmf{gluon,label=$g$,label.side=left}{v1,b}
      \end{fmfgraph*}
    \end{fmffile}

    \begin{fmffile}{g4}
      \begin{fmfgraph*}(40,25)
        \fmfleft{i1,i2}
        \fmfright{o1,o2}
        \fmf{gluon,label=$g$,label.side=left}{i1,v1}
        \fmf{gluon,label=$g$,label.side=left}{i2,v1}
        \fmf{gluon,label=$g$,label.side=left}{v1,o1}
        \fmf{gluon,label=$g$,label.side=left}{v1,o2}
      \end{fmfgraph*}
    \end{fmffile}

    \caption[The vertices of QCD.]{
      The Standard Model Lagrangian contains terms in which appear (upper) one of the quark fields twice, and a gluon field once, (middle) 3 separate gluon fields, and (lower) 4 gluon fields.
      These are the vertices of QCD.      
      Unlike the lines of other bosons, gluon lines are traditionally drawn as springs.
    }
    \label{fig:qcdvertices}
  \end{figure}  

  Finally, some of the vertices of the weak interaction are shown in Figure~\ref{fig:weakvertices}.

  \begin{figure}[h!]
    \centering
    \begin{fmffile}{qqW}
      \begin{fmfgraph*}(40,25)
        \fmfleft{i1,i2}
        \fmfright{o1,o2}
        \fmfbottom{b}
        \fmf{fermion,label=$q_u$,label.side=left}{i2,v1}
        \fmf{fermion,label=$q_d$,label.side=left}{v1,o2}
        \fmf{photon,label=$W$,label.side=left}{v1,b}
      \end{fmfgraph*}
    \end{fmffile}

~

    \begin{fmffile}{lnuW}
      \begin{fmfgraph*}(40,25)
        \fmfleft{i1,i2}
        \fmfright{o1,o2}
        \fmfbottom{b}
        \fmf{fermion,label=$\ell$,label.side=left}{i2,v1}
        \fmf{fermion,label=$\nu_{\ell}$,label.side=left}{v1,o2}
        \fmf{photon,label=$W$,label.side=left}{v1,b}
      \end{fmfgraph*}
    \end{fmffile}

~

    \begin{fmffile}{ffZ}
      \begin{fmfgraph*}(40,25)
        \fmfleft{i1,i2}
        \fmfright{o1,o2}
        \fmfbottom{b}
        \fmf{fermion,label=$f$,label.side=left}{i2,v1}
        \fmf{fermion,label=$f$,label.side=left}{v1,o2}
        \fmf{photon,label=$Z$,label.side=left}{v1,b}
      \end{fmfgraph*}
    \end{fmffile}

~

    \begin{fmffile}{ffH}
      \begin{fmfgraph*}(40,25)
        \fmfleft{i1,i2}
        \fmfright{o1,o2}
        \fmfbottom{b}
        \fmf{fermion,label=$f$,label.side=left}{i2,v1}
        \fmf{fermion,label=$f$,label.side=left}{v1,o2}
        \fmf{dashes,label=$h$,label.side=left}{v1,b}
      \end{fmfgraph*}
    \end{fmffile}

    \caption[Some vertices of the weak interaction.]{
      The Standard Model Lagrangian contains terms in which appear (upper) an up-type quark field and down-type quark field once each, and the W field once, (second) a similar diagram with a charged lepton and its associated neutrino replacing the quarks, (third) a fermion field appears twice, and the Z field once, and (lower) a non-neutrino fermion field appears twice, and the Higgs once.
      These are the vertices of the weak interaction that include both fermions and bosons.      
      The weak interaction also includes many boson-only interactions similar to Figure~\ref{fig:qcdvertices} (lower), but these are omitted for brevity.
      The Z diagram (third) is very similar to the interaction of the photon with fermions, with the important addition that the fermion here can also be an electrically-neutral neutrino.
      The lines of spin-0 bosons are traditionally drawn as dotted lines, as in the lower diagram.
    }
    \label{fig:weakvertices}
  \end{figure}  

  In order to calculate rates and distributions of elementary particle physics processes, one assembles diagrams using the minimal number of vertices, then the next to minimal number, and so forth up to the desired precision, then follows an algorithm to convert each diagram to its equivalent mathematical expression.
  The result of evaluating this expression is the quantum mechanical amplitude for the process.
  This tedious procedure is now performed almost entirely by computers, using software such as the MadGraph generator \cite{madgraph}.

  \subsection{Quantum Chromodynamics, Hadronization, and Jets} \label{sec:hadronization}

  Quantum Chromodynamics is the theory of the strong interactions incorporated into the Standard Model, describing the interactions of quarks and gluons.
  States composed of quarks that are neutral with respect to the strong interactions (``colorless,'' to avoid confusion with electromagnetic neutrality) are called hadrons, analogous to atoms of electromagnetism.
  There are two distinct ways to create a colorless state, due to the unique 3-component structure of the QCD charge.
  The first is to have a quark and antiquark with +1 and -1 units, respectively, of the same component of the QCD charge.
  These mutually cancel like a proton and electron in electromagnetism, forming a hadron called a meson.
  The second is to have 3 quarks, each with 1 unit of a different component of the QCD charge, or 3 antiquarks each with -1 unit of a different component.
  This kind of hadron is called a baryon, and there is no electromagnetic analogue to this kind of neutral object.
  The only stable hadron is the proton, the lightest baryon, excepting neutrons bound to protons by the QCD equivalent of dipole forces in atomic nuclei.
  The formation of hadrons is called hadronization, and the hadronization process after a high energy collision tends to produce objects called jets that are important in experiments.
  
  To understand hadronization and jets, it is necessary to understand a feature of the strong interaction intimately related to why it is short ranged despite having a massless mediator, as was mentioned in the previous sections.
  Unlike the familiar electromagnetism, in which the potential energy associated with two charges decreases as the separation of the changes increases, the QCD potential energy increases without bound as two quarks are separated, roughly linearly proportional to distance \cite{lattice_potential}, as shown in Figure~\ref{fig:QCDpotential}.
  Eventually, the potential energy stored in the quark system exceeds the rest energy of a new hadron.
  At this threshold, it becomes energetically preferable to convert some of the stored energy into new hadrons, creating a new quark-antiquark pair in order to reset the quark separation distance, than to allow further separation of the quarks.
  As a result, any attempt to separate two quarks cannot ultimately succeed, and all quarks are bound inside colorless hadrons. 
  As gluons {\it also} carry color charge (that is, gluons interact with themselves as shown in Figure~\ref{fig:qcdvertices}), they are similarly confined, and the reach of the strong interaction is limited to only a few femtometers.

  \begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/lattice_potential_qcd.pdf}
    \caption[Tracker material budget.]{
      The potential energy associated with two static quarks predicted by QCD as a function of distance, calculated numerically using the lattice technique.
      The potential increases roughly linearly with distance once a quark is no longer inside a hadron.
      The strong coupling used for the numerical calculation is denoted by $\beta$.
      Taken from \cite{lattice_potential}.}
    \label{fig:QCDpotential}
  \end{figure}  

  During a high energy collision involving a hadron like a proton, quarks and gluons are customarily ejected, and eventually reach a great enough distance to trigger the formation of new hadrons.
  But, the ejection is oftentimes so violent that even this new hadron fragments into new daughter hadrons, then these fragment, and so on.
  This fragmentation process produces a spray of hadrons, hadron decay products, and hadrons formed from QCD radiation analogous to bremsstrahlung, all traveling in approximately the same direction as the original ejected quark or gluon, which are collectively called a hadron jet.

  Unfortunately, while the production of jets is understood qualitatively, it is difficult to predict quantitatively due to yet another peculiar feature of QCD.
  The perturbative expansion described in the previous section is performed in powers of the interaction's coupling, with one factor of the coupling per vertex.
  In order for a truncated perturbation series to be a good approximation, the coupling must be significantly less than 1.
  Otherwise, diagrams with more vertices, representing higher order terms in the expansion, are in general more important than diagrams with fewer vertices, and no finite truncation can be accurate.
  Although the QCD coupling is less than 1 at high energy, making the perturbative approach still viable for very high energy collisions, the coupling explodes at low energies, which earns the strong interaction its name.
  Even at intermediate energies where QCD is technically perturbative, the number of diagrams that must be evaluated for an accurate result is still impractically large.

  \begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/qcd_coupling.pdf}
    \caption[The QCD coupling as a function of the interaction energy.]{
      The QCD coupling explodes at low energy, making QCD non-perturbative for any interaction energy below around 1~GeV.
      The different lines are obtained for different methods of performing the calculation, specifically different renormalization schemes.
      Taken from \cite{qcd_coupling}.}
    \label{fig:QCDcoupling}
  \end{figure}  

  Non-perturbative approaches to QCD exist, including the lattice numerical method used to produce the plot shown in Figure~\ref{fig:QCDpotential}, but they are extremely computationally expensive for even the most simple calculations.
  It is not feasible to perform non-perturbative calculations of low energy QCD physics from first principles in an environment as complex as high energy hadron collisions.
  For the hadronization and fragmentation process, heuristic approximations tweaked to match data are used instead, typically the Lund String Model as implemented in the Pythia software package (see Section~\ref{sec:simulation}).
  Additionally, relatively low energy quarks and gluons can be emitted during the collision itself, called Initial State Radiation (ISR).
  A diagram of $u\bar{u} \rightarrow Z \rightarrow e^+e^-$ that includes a single ISR gluon is shown in Figure~\ref{fig:ISRdiag}.
  The most reliable way to predict ISR is to measure it in a very similar physics process, then convert this measurement to an expected rate in the physics process of interest.
  For example, ISR in $Z\rightarrow e^+e^-$ events is essentially identical to ISR in $Z\rightarrow \nu\nu$ events.
  
  \unitlength=2mm
  \begin{figure}[h!]
    \centering
    \begin{fmffile}{ISRdiag}
      \begin{fmfgraph*}(40,25)
        \fmfleft{i1,i2}
        \fmfright{o1,o2}
        \fmfbottom{b}
        \fmf{fermion,label=$u$,label.side=left}{i2,v1}
        \fmf{fermion,label=$\bar{u}$,label.side=left}{i1,vi}
        \fmf{fermion}{vi,v1}
        \fmf{gluon,label=ISR gluon,label.side=left}{vi,b}
        \fmf{photon,label=$Z$,label.side=left}{v1,v2}
        \fmf{fermion,label=$e^-$,label.side=left}{o2,v2}
        \fmf{fermion,label=$e^+$,label.side=left}{v2,o1}
      \end{fmfgraph*}
    \end{fmffile}

    \caption[A Feynman diagram including an ISR gluon.]{
      Initial State Radiation emitted as part of the hard collision, in this case $u\bar{u} \rightarrow Z \rightarrow e^+e^-$, can be at relatively low energy.
      This gluon will hadronize and add a jet to the event.
      QCD ISR is difficult to predict accurately due to the intractability of QCD at low energy.
    }
    \label{fig:ISRdiag}
  \end{figure}  
  \unitlength=2mm

  In summary, QCD's intractability at low energy combined with its raw strength make precise predictions of the outcome of hadron collisions difficult to obtain, despite the Standard Model's overall success in describing the interactions of elementary particles.
  The Standard Model prediction cannot always be obtained directly from first principles with satisfactory accuracy.

  \subsection{Parton Distribution Functions} \label{sec:PDFs}

  Being composite particles bound by QCD, the internal structure of hadrons is rich and difficult to understand quantitatively, due to the impossibility of using perturbation theory at the low energies that prevail in hadrons.
  Of greatest interest is the internal structure of the proton, the only stable hadron.

  The proton is often said to be a bound state of two up quarks and one down quark.
  This is true enough---the {\it net} composition of a proton is two up quarks and one down quark---but is an oversimplification.
  The quarks in a proton constantly emit and absorb gluons, which can split to quark-antiquark pairs, which can in turn annihilate back to gluons.
  Viewed at a small enough length scale, the proton is not a simple object, and not even a bound state of 3 quarks, but rather a complex cloud of quarks and gluons with various energies.
  This is problematic for any attempt to predict the outcome of proton collisions, because one cannot be sure which of these subcomponents, called partons, will be the component of the proton that actually experiences a collision.
  Due to the intractability of QCD at low energy, the composition of a proton is known only from a fit to data, and called a Parton Distribution Function (PDF).
  PDFs are typically expressed as the probability to find a parton of a given species carrying a given fraction of the proton's energy, as shown in Figure~\ref{fig:PDF}.
  These PDFs display a few sensible features described in the figure's caption.
  A large number of proton collisions collectively sample all the possible initial partons, with momentum distributions described by the PDFs.
  Thus, a dataset composed of proton collisions naturally probes a wide range of collision energies with a variety of colliding particles, making proton collisions well-suited to searches for new particles of unknown properties.

  \begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/parton_distribution_function.pdf}
    \caption[Proton parton distribution function.]{
      The plot shows the measured parton distribution function for a proton at an interaction energy of 100~GeV.
      The horizontal axis $x$ is the fraction of the proton's total momentum, and the vertical axis is the probability distribution.
      The finite thickness of the curves indicates the uncertainty.
      As expected for a proton, $uud$, the probability to collide with an up quark is around twice the probability to collide with a down quark at any energy.
      The $v$ subscript on these two curves indicate that these are the ``valence'' quarks, the only quarks that are present as more than transient products of gluon splitting.
      It is much less likely to find an up or down antiquark, and roughly equally likely to find either, because these antiquarks are produced only by gluons splitting to quark-antiquark pairs.
      Since they are also produced only transiently by gluon splitting, the probability to find a heavier quark, namely $s$, $c$, or $b$, is exactly equal to the probability to find a heavy antiquark.
      Since the heavy quark and antiquark curves overlap, only the quark curves and represent both.
      Quarks aside, notice that the gluon curve (red) has been suppressed by a factor of 10 to fit on the plot.
      To a decent approximation, high energy proton colliders in practice collide gluons.
      Taken from \cite{PDFs}.}
    \label{fig:PDF}
  \end{figure}  


\section{Some Problems with the Standard Model} \label{sec:SMproblems}

The Standard Model is extremely successful, as epitomized by the result in Table~\ref{tab:electronmu}, but not entirely satisfactory.
In fact, its success in laboratory experiments is a source of frustration for elementary particle physicists, as there are precious few hints as to how its deficiencies can be repaired.
This section introduces a few of the known problems with the Standard Model.

  \subsection{Gravity, and the Standard Model as an Effective Field Theory} \label{sec:gravity}

  In Section~\ref{sec:standardmodel}, there is never any mention of gravity as an interaction between particles.
  This is because the Standard Model entirely omits gravity from its description of nature, which means the Standard Model cannot possibly be the final theory.
  Instead, the Standard Model must be only an {\it effective theory}, a low energy approximation of nature that must be replaced by a more fundamental theory at some cutoff energy.
  In the case of the Standard Model, this cutoff cannot be any larger than the energy at which gravity becomes relevant in the interactions of elementary particles, called the Planck scale.

  Unlike the fields of the Standard Model, whose couplings are dimensionless, the gravitational coupling $G$ has dimensions of inverse energy squared.
  An effective field theory of the weak interactions developed by Enrico Fermi, which models the weak interactions of fermions without mediating bosons, has a similar coupling, $G_F \approx \frac{1}{(100\mathrm{~GeV})^2}$.
  The energy scale embedded in the coupling is a message from nature, hinting that this effective theory must be replaced by something more fundamental in order to model physics at the scale of around 100~GeV or more.
  Nowadays, we recognize this energy as the mass scale of the weak interaction's mediating bosons.
  At interaction energies on the order of 100~GeV or larger, there is enough energy to produce the boson itself as a resonant excitation of the field, a real particle, and the underlying theory is revealed.
  Making an analogous inference from the gravitational coupling, one obtains the maximum possible cutoff energy for the Standard Model, the Planck energy of around $10^{19}$~GeV, at which gravity must be incorporated into the quantum theory.

  Moreover, the Standard Model becomes self-inconsistent at very high energies, even ignoring the gravity problem \cite{landaupole,higgstriviality}.
  The couplings of both electromagnetism and of the Higgs to itself increase with increasing energy, until eventually diverging at very high energy scales, a theoretical consistency issue called quantum triviality.
  As a result, the Standard Model's formulation of electromagnetism cannot be used for any energy greater than around $10^{34}$~GeV~\cite{landaupole}.
  Although this energy is far greater than the Planck energy, at which the Standard Model must fail anyway due to its exclusion of gravity, it is a completely independent indication that the Standard Model must be only a low energy limit of some more fundamental theory that supersedes it at high energy, one that does not exhibit these kinds of problems.
  Even if one supposes that gravity's exclusion from the Standard Model is not actually a problem, that attempting to treat gravity like the other interactions is misguided, the Standard Model still cannot be the final theory.

  The Higgs self-interaction similarly diverges above the Planck scale, although had the Higgs mass turned out to be only around 200~GeV rather than the measured 125~GeV, the problem would arise below the Planck scale, and a Higgs mass as small as around 600~GeV would have made the triviality scale only a few TeV, accessible to current experiments \cite{higgstriviality}.

  Thus, there exist known finite energy scales at which the Standard Model {\it must} fail, due both to its exclusion of gravity and to internal mathematical consistency issues, and the actual cutoff energy may be much smaller than these upper limits.
  Every experiment probing an unexplored energy scale has a chance to be the first to observe a hint as to what this more fundamental theory looks like.

  \subsection{The Hierarchy Problem} \label{sec:hierarchy}

  As described in the previous section, the energy scales of the weak interaction and of gravity differ by approximately 17 orders of magnitude.
  If the Standard Model emerges from some more fundamental theory at the Planck energy, then what effect has pushed the weak scale down 17 orders of magnitude?
  The most troublesome expression of this apparently inexplicable hierarchy of energy scales centers on the mass of the Higgs boson.
  In the Standard Model, the Higgs mass $m_h$ is given approximately by \cite{SUSYnaturalness,higgsmass},
  \begin{equation} \label{eqn:higgsmass}
    m_h^2 \approx 2\mu^2 + (\Delta m_h)^2
  \end{equation}
  where $\mu$ is a free parameter called the bare mass of the Higgs that would most naturally be 0, and $\Delta m_h$ is an adjustment that the Higgs acquires via its interactions with other fields in the Standard Model,
  \begin{equation} \label{eqn:masscorrection}
    \Delta m_h^2 \approx \frac{3}{4\pi^2}\left(\lambda^2 - \lambda_t^2 + \ldots\right)\Lambda^2
  \end{equation}
  where $\lambda$ is the Higgs' coupling to itself, $\lambda_t$ is its coupling to the top quark, and $\Lambda$ is the energy at which the Standard Model is replaced by a more fundamental theory that is assumed not to contribute any further corrections to the Higgs mass, at most the Planck Energy.
  The couplings of the Higgs to all the fermions of the Standard Model appear in the correction with the same form as the top's $\lambda_t$, but the coupling of the top quark is by far the most important, since at $\sim$1 it is by far the largest, as can be seen from the mass column in Table~\ref{tab:fermions}.
  The Higgs' self-coupling is somewhat smaller \cite{higgsmass}.
  Therefore, $\Delta m_h^2 \sim -\Lambda^2$.
  If one takes $\Lambda \approx 10^{19}$~GeV, the Planck Energy, then Equation~\ref{eqn:higgsmass} is deeply implausible after also inserting the experimental value $m_h \approx 125$~GeV,
  \begin{equation}
    (125)^2 \approx 2\mu^2 - (10^{19})^2.
  \end{equation}
  The Higgs mass of 125~GeV emerges from almost perfect cancellation of two terms with values around $10^{19}$~GeV.
  A model that produces an output of a very different order of magnitude from the generating input is said to be finely tuned.
  The Standard Model, epitomized by the Higgs mass, is {\it egregiously} finely tuned.
  This is the Hierarchy Problem.

  Of course, the Standard Model's cutoff need not be the Planck Energy. 
  If $\Lambda$ is only 1000~GeV, then the Hierarchy Problem as stated disappears, and one can expect to discover new physics in elementary particle interactions at this experimentally accessible scale.
  Still, merely not being the Standard Model is insufficient; the replacement theory also needs to introduce a mechanism to stabilize the Higgs mass, lest it make its own contributions or allow the Standard Model fields to continue pushing it to large values.
  By far the most popular proposal for the kind of model that may supersede the Standard Model at the TeV scale is called Supersymmetry, discussed in Section~\ref{sec:SUSY}.

  \subsection{Massive Neutrinos} \label{sec:neutrinomasses}

  In the Standard Model, all fundamental particles acquire mass through their interactions with the Higgs, and the neutrinos are strictly massless.
  However, recent measurements of neutrinos have established that they undergo flavor oscillation, whether produced by cosmic rays in the atmosphere \cite{atmospheric_neutrinos}, or by nuclear processes in reactors on Earth \cite{reactor_neutrinos} or in the Sun's core \cite{solar_neutrinos}.
  This means that, just as for quarks, the neutrino mass eigenstates are not equal to the interaction eigenstates, and more importantly that the neutrinos' mass eigenstates are not equal {\it to each other} \cite{nufit}.
  This is only possible, of course, if at least 2 of the 3 neutrinos are massive.
  Although difficult particles to measure, experiments have managed to map out the neutrino mixing matrix analogous to the CKM matrix for quarks, the Pontecorvo-Maki-Nakagawa-Sakata matrix \cite{pdg},
  \begin{equation}
    U_{PMNS}
\approx
    \begin{bmatrix} 
      0.800 \leftrightarrow 0.844 & 0.515 \leftrightarrow 0.581 & 0.139 \leftrightarrow 0.155 \\
      0.229 \leftrightarrow 0.516 & 0.438 \leftrightarrow 0.699 & 0.614 \leftrightarrow 0.790 \\
      0.249 \leftrightarrow 0.528 & 0.462 \leftrightarrow 0.715 & 0.595 \leftrightarrow 0.776 
    \end{bmatrix}
  \end{equation}
  where each element of the matrix shows the range consistent with experiment.
  Mysteriously, $U_{PMNS}$ is not approximately diagonal, unlike the CKM matrix.
  Nonzero neutrino masses are entirely inconsistent with the Standard Model and, to date, are the only observables measured in laboratories that the Standard Model has failed to predict accurately.

  \subsection{Astrophysical Evidence of Dark Matter} \label{sec:DMevidence}

  Over the last century, astrophysical observations have assembled conclusive evidence that most of the Universe's gravitating mass has at most a neutrino-like interaction cross section.

  Some of the oldest evidence was obtained from galactic rotation curves.
  In general, stars far from the cores of galaxies have much greater orbital velocity than would be inferred from the visible mass to their interior, implying either that general relativity is not a good model of gravity on large distance scales, or that most of the gravitating mass in galaxies is weakly interacting and arranged as an extended halo \cite{rotationcurves}.
  Recent observations have concluded with good precision that this is true also in the Milky Way \cite{rotationcurves_MW}.

  Observation of the Cosmic Microwave Background (CMB) has allowed for measurement of the Universe's energy density, and the forms taken by that energy density.
  The best fit is a Universe composed of about 6 times as much ``dark'' matter as visible matter, consistent with the ratio that would explain galactic rotation curves \cite{planckCMB}.
  No modification of gravity has ever been found that can satisfactorily explain both the rotation curve observations, and explain these energy density measurements.

  Observations of the Universe at more recent times have found a few cases of colliding galaxy clusters, most famously the Bullet Cluster, that offer further evidence for the existence of dark matter.
  In galaxy cluster collisions, the luminous matter can collide and produce a very hot plasma due to its relatively strong interactions, while dark matter continues on unaffected.
  The luminous matter can be accounted for directly via the emissions of the hot plasma, while the location of the mass, visible or not, can be measured by observing the gravitational lensing of the light of more distant galaxies by the foreground colliding clusters \cite{bulletcluster}.
  The result of this analysis for the Bullet Cluster is shown in Figure~\ref{fig:bulletcluster}.
  The visible matter, revealed by its x-ray emissions, is caught in the collision zone at the center, while the majority of the gravitating matter has continued on unaffected, and is detectable only through its powerful gravitational lensing on either side of the luminous plasma.
  The inferred ratio of dark matter and visible matter is consistent with the amount that would explain both rotation curves and the energy densities measured using the CMB.
  Again, no modification to gravity has been found that can simultaneously explain galactic rotation curves and the energy density measurements from the CMB, and also the gravitational lensing observed in the Bullet Cluster and similar collisions.
  \begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/bulletcluster.pdf}
    \caption[Mass and x-ray distributions of the Bullet Cluster.]{
      The distribution of luminous matter emitting x-rays (black dots) and the location of mass inferred through gravitational lensing of background sources (contours) are offset.
      The plasma emitting x-rays has been slowed and shocked as a result of the astronomically recent collision between the galaxy clusters, and so has been left behind by the dark matter, which constitutes most of the mass and was unaffected by the collision due to its negligible interaction cross section.
      Taken from \cite{bulletcluster}.}
    \label{fig:bulletcluster}
  \end{figure}  

  Finally, astronomers have recently identified a few small galaxies that appear to contain little or no dark matter \cite{zeroDMgalaxy}.
  While this is an extraordinarily rare occurrence and effectively impossible in a large galaxy like the Milky Way, small galaxies are subject to larger statistical fluctuations, and the Universe has a rather large sample size of galaxies.
  It is difficult to imagine a modification of gravity that could, in most observations, cause there to appear to be around 6 times are much invisible mass in the Universe as visible mass, but in some small galaxies appear as if there is no modification at all.

  If dark matter is not composed of a weakly interacting particle beyond the Standard Model, the only remaining possibility is that it is composed of larger non-radiating masses like rogue planets or black holes produced in the high-density primordial Universe and surviving to the present day.
  This possibility is taken seriously, and can be investigated by searching for small microlensing events, where the light of a distant star is briefly gravitationally lensed when one of these bodies passes between an Earth-based telescope and the star.
  As there is around 6 times as much dark mass as visible in the galaxy, such an event should be relatively common.
  No excess of such events is observed in microlensing and complementary searches, so that if all dark matter has exactly the same mass, it cannot have any mass in the range $10^{-7}M_{\mathrm{Sun}} < M_{DM} < 10^5 M_{\mathrm{Sun}}$ \cite{primordialBH_dist}.
  This leaves open the possibility that dark matter could be composed of black holes of small mass, with Schwarzschild radius around 0.1~mm or less, or that it could be composed of bodies with a range of masses.
  However, both possibilities are disfavored.
  It is suspected that small primordial black holes would generically be captured by stars, eventually migrate to the core as the star dies, and destroy the compact remnant via accretion \cite{primordialBH}, but no such events are observed.
  As for the latter possibility, the data places such strong constraints on the acceptable mass distribution that it is necessary to construct one carefully by hand to avoid the limits \cite{primordialBH_dist}.
  Particle dark matter, despite its own history of non-observation, remains the possibility most easily reconciled with the data.

  There is especially strong hope for detecting particle dark matter at the TeV scale, due to a suggestive coincidence called the Weakly Interacting Massive Particle (WIMP) Miracle.
  The characteristic energy scale of the weak interaction is $\sim100$~GeV.
  A hypothetical particle with mass in the suitable range, on the order of 1 to 10,000 GeV, interacting via the Standard Model's weak interaction or a similar interaction, would have been produced in the early Universe and survived to the present day at the proper density to account for the observed cold dark matter \cite{WIMPmiracle}.
  Furthermore, it would be an astonishing coincidence if the dark matter particle is only distantly related to the Standard Model, perhaps only by gravitational interactions, and yet somehow by dumb luck ended up with a relic mass density on the same order of magnitude as that of the Standard Model particles.
  Viewed in light of indications that new physics ought to be found at the TeV scale from the Hierarchy Problem and other issues with the Standard Model, the case for particle dark matter at the TeV scale is compelling.
  The chief argument against WIMPs is that they have not yet been observed in experiments despite concentrated efforts, yet much of the parameter space remains untouched.
  It is a high priority of current experiments to extend sensitivity into these regions.

\section{Supersymmetry} \label{sec:SUSY}

Among all the proposals for extending the Standard Model and repairing many of its issues at the TeV scale, supersymmetric extensions are by far the most popular and intensively researched.
Supersymmetry proposes that for every Standard Model fermion (boson), there exists a {\it superpartner} boson (fermion) with nearly identical properties aside from its spin.
The superpartners of spin-$\frac{1}{2}$ particles are spin-0, and the superpartners of spin-1 and spin-0 particles are spin-$\frac{1}{2}$.
For example, the superpartner of the electron is a spin-0 particle called the scalar electron, usually contracted to ``selectron.''
The fermionic superpartners of bosons instead receive the ``-ino'' suffix, for example ``higgsino.''
If supersymmetry is realized at the TeV scale, it would resolve many of the Standard Model's problems, and would be experimentally testable using TeV-scale particle colliders.
This section introduces some of the theoretical reasons for the popularity of supersymmetry, and how it may be targeted experimentally.

  \subsection{Theoretical Appeal} \label{sec:SUSYappeal}

  A full discussion of the theoretical appeal of supersymmetry is beyond the scope of this thesis, so this section briefly discusses only a pair of the most compelling points that are most closely connected to why supersymmetry at the TeV scale is of great interest.
  First, supersymmetry provides a well-motivated dark matter candidate, a concrete realization of the WIMP Miracle.
  Second, it resolves the Hierarchy Problem and is almost certainly the only solution that possibly can; if supersymmetry is not realized at the TeV scale, it likely means that the Standard Model's fine tuning has somehow been misinterpreted and is not in fact a problem.

  Supersymmetry's dark matter candidate arises as a side effect of a solution to an unrelated problem.
  Naive implementations of supersymmetry make possible the decay $p \rightarrow \pi^0~e^+$ via mediation by a squark (the pion is the lightest meson and the only hadron lighter than the proton).
  The proton would be highly unstable for any reasonable squark mass, in conflict with the lower limit on this decay mode of some $10^{32}$ years, and the any-mode limit of more than $10^{29}$ years \cite{pdg}.
  However, it is possible to have a stable proton within supersymmetric models, by proposing that there exists a conserved multiplicative quantum number called R-parity and assigning superpartners odd R-parity and Standard Model particles even R-parity.
  This saves the proton, but as a side effect also makes the lightest supersymmetric particle (LSP) stable, since decays to final states including other superpartners are kinematically impossible and decays to final states containing only Standard Model particles cannot conserve R-parity \cite{pdg}.

  At first, LSP stability appears to be a problem as severe as proton decay.
  If the LSP is stable, then there should be a large relic population persisting from the primordial plasma, indeed this relic population may represent the majority of the mass of the Universe, but no such population has ever been observed.
  However, astrophysical observations have assembled a compelling case that a majority of the mass of the Universe is not of the Standard Model, making it possible that stable LSPs {\it have} been observed, as dark matter, via their gravitational interactions.
  Of course, this position is only tenable if one of the superpartners has the properties necessary to be a dark matter candidate.
  The superpartners of neutrinos, the sneutrinos, are an obvious first guess, but their interaction cross sections are too large in generic implementations of supersymmetry, so that a sneutrino LSP requires a special purpose-built model to be viable \cite{sneutrinos}.
  Fortunately, the neutral superpartners of the weak bosons, the neutralinos, could have even smaller interaction cross sections than sneutrinos, and are ideal dark matter candidates.
  Moreover, as part of supersymmetry's solution to the Hierarchy Problem, the neutralinos in general and especially the higgsino are constrained to be relatively light \cite{distracksAMSB, SUSYnaturalness, naturalWIMP}, making a neutralino LSP plausible on more than phenomenological grounds.

  The supersymmetric solution to the Hierarchy Problem is founded on a simple observation.
  In Equation~\ref{eqn:masscorrection}, the Higgs' coupling to itself, a boson, and to the top quark, a fermion, appear with opposite signs.
  This is general: the contributions of bosons and of fermions to the Higgs mass have opposite signs.
  Among the Standard Model fields, this is not very helpful, since the top quark simply dominates the other fields.
  But, if every boson had a fermion counterpart with exactly the same properties and vice versa, all contributions to Equation~\ref{eqn:masscorrection} would exactly cancel.
  Of course, experiments have long excluded the existence of, say, a selectron of the same mass and interactions as the electron, so supersymmetry must be spontaneously broken if it exists, with the superpartners pushed to a higher mass scale.
  A variety of well-motivated mechanisms for this have been proposed \cite{GMSB_theory,distracksAMSB}.
  Whatever the mechanism, the scale of supersymmetry breaking would lead directly to the observed Higgs mass, implying that superpartners and especially the higgsino should begin to appear at energies not far beyond the observed Higgs mass \cite{distracksAMSB,GMSB_theory,SUSYnaturalness,naturalWIMP}.

  \subsection{Experimental Signatures} \label{sec:SUSYexp}

  Since every supersymmetric particle must decay to the nearly undetectable LSP, the primary signature of superpartner production in colliders is large missing energy and momentum, carried away by the undetectable LSP.
  Additionally, conservation of R-parity requires that superpartners be produced in pairs, so that these events are characterized by {\it two} heavy invisible particles.
  As experimental constraints on the visible superpartner masses increase, the likelihood that the decays are extremely energetic also increases.
  Two particles with large masses of perhaps 2~TeV would be produced only rarely in state of the art accelerators, but their events would be extraordinarily high energy, unless the LSP mass is also large and absorbs most of the decay energy.

  By definition, the LSP is the most energetically accessible superpartner.
  Even so, the preferred supersymmetry search strategy is to use the invisibility of the LSP to identify decay chains of more massive but more strongly interacting superpartners, as the LSP itself must  have a very low production rate due to its weak interaction cross section, if it exists.
  At hadron colliders, the best candidate superpartners are those participating in the strong interactions, squarks and gluinos, due to the relatively large production cross sections for strongly interacting particles (see Figure~\ref{fig:SUSYxsec}). 
  Unfortunately, while the mass of the LSP is restricted to a relatively low value if supersymmetry is to solve the Hierarchy Problem, the masses of other superpartners are not as strongly constrained.
  Attempts to observe the LSP in decays of more easily produced superpartners require that these intermediate states have accessible masses, which is not necessarily the case for any experiment to date \cite{SUSYnaturalness,naturalWIMP}.
  It may be necessary to search for direct LSP production if the other superpartners are too massive, which will require a vast dataset due to the low production rate.
  As the present datasets are still too small for highly sensitive direct LSP searches, most current analyses are two-dimensional, targeting a relatively easily pair-produced heavy superpartner and the LSP ultimately produced at the end of its decay chain.

  In some supersymmetric models, a relatively large supersymmetry energy scale tends to suppress the mass splitting of the lightest chargino \chionepm and lightest neutralino \lsp, for instance in the model described in \cite{distracksAMSB},
  \begin{equation}
    \frac{\Delta m}{m} \sim \left(\frac{m_W}{\mu}\right)^4
  \end{equation}
  where $\frac{\Delta m}{m}$ is the fractional splitting of the nearly-degenerate \chionepm and \lsp masses and $\mu$ is a parameter appearing in the supersymmetric Higgs potential, which is partially responsible for setting the mass scale of superpartners.
  A $\mu$ value of a few hundred GeV would produce a mass splitting of one part per thousand or less, as small as a few hundred MeV.
  As the viable masses of superpartners are pushed to larger values by experimental constraints, such a scenario becomes increasingly plausible.
  At a splitting of a few hundred MeV, the phase space of the decay $\chionepm \rightarrow \pi^{\pm} \lsp$ is so small that the lifetime becomes sufficient to produce macroscopic decay lengths.
  Furthermore, with only a few hundred MeV available to the pion, it would fall below the minimum \pt for track reconstruction, making the decay of \chionepm completely invisible.
  As \chionepm has a macroscopic decay length and nonzero electric charge, this invisible decay can produce the remarkable experimental signature called a disappearing track.
  Interest in this long-lived \chionepm model and other models that include long-lived particles has recently grown, motivated in part by the growing theoretical plausibility, and in part by the additional sensitivity that exploitation of long-lived particle signatures can provide.
  The disappearing track signature in particular is used by the search in Section~\ref{sec:distracks}.

  \subsection{Simplified Models} \label{sec:SUSYsms}

  Due to the large number of particles and free parameters in even the simplest supersymmetric extensions of the Standard Model, it is not possible to make exact calculations that hold for every possible realization of supersymmetry, and every supersymmetry simulation is to some extent dependent on the exact choices of free parameters.
  To make calculations tractable, the community has widely adopted use of simplified models \cite{SMS}.
  Simplified models are effective field theories that assume all superpartners except those of interest are at an inaccessible mass scale.
  Effectively, instead of extending the Standard Model with every superpartner simultaneously, only a few particles, perhaps only the gluino and the LSP, are added.
  This makes it possible to calculate pair-production cross sections as a function of mass, allows for the decay channel(s) to be set without worrying about the model's self-consistency, and so on without excessive computational effort.
  The gluino pair-production cross section as a function of mass in such a simplified model is shown in Figure~\ref{fig:SUSYxsec}.
  \begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/gluino_xsec.pdf}
    \caption[Theoretical gluino pair production cross section in simplified models.]{In simplified models, it is possible to calculate superpartner production cross sections from theory.
Here, the theoretical gluino pair production cross section in 13 TeV proton-proton collisions is shown in black, with the one standard deviation uncertainty shown as a yellow band.
The cross section drops rapidly with increasing mass.
Based on cross section values used in \cite{MT2_2019}, calculated in \cite{SUSYxsecs}.
Compare Figure 5 (upper) from \cite{SUSYxsecs}, of which this is a simplified reproduction.
}
    \label{fig:SUSYxsec}
  \end{figure}  
  Results expressed in these models are straightforwardly reinterpreted as general constraints on more realistic models with more complicated phenomenologies.

\section{Other Models} \label{sec:othermodels}

While supersymmetry is generally of greatest interest and so drives the design of many searches for new physics, it shares its primary signatures with some other hypothetical extensions of the Standard Model.
Two are worth special mention, since they are considered explicitly by the analysis discussed in Section~\ref{sec:MT2classic}.

Some models propose the existence of ``leptoquarks,'' strongly-interacting bosons that have vertices in which one fermion is a lepton and one is a quark, with one example shown in Figure~\ref{fig:LQvertex}.
These models have recently attracted some interest due to their ability to explain some minor anomalies in a few rare meson decays \cite{LQhunter,minorBanomaly,Banomaly}.
As leptoquarks can be pair-produced and decay to a neutrino and a quark, they produce a very similar experimental signature to a squark decaying to a quark and LSP, in the limit where the LSP is nearly massless.
Therefore, searches for squarks decaying to LSPs can easily be reinterpreted as searches for leptoquarks.

\begin{figure}[h!]
  \centering
  \begin{fmffile}{LQvertex}
    \begin{fmfgraph*}(40,25)
      \fmfleft{i1,i2}
      \fmfright{o1,o2}
      \fmfbottom{b}
      \fmf{fermion,label=$q$,label.side=left}{i2,v1}
      \fmf{fermion,label=$\nu$,label.side=left}{v1,o2}
      \fmf{scalar,label=LQ,label.side=left}{v1,b}
    \end{fmfgraph*}
  \end{fmffile}
  \caption[A defining leptoquark vertex.]{
    Leptoquarks are defined by vertices that connect quarks and leptons. 
    In this example, the lepton is a neutrino.
  }
  \label{fig:LQvertex}
\end{figure}  


A second model, this time motivated by a small excess in events with large missing energy and relatively low energy, proposes a generic strongly-interacting scalar $\phi$ mediating Standard Model interactions with a fermionic dark matter candidate $\psi$ \cite{monophi} as shown in Figure~\ref{fig:monophidiag}, with parameters tuned to match the minor observed anomaly.
This model need not involve pair-production and could in principle have a smaller mass scale than supersymmetry, and so can produce events with lower energy and lower numbers of hadronic jets than would be typical for supersymmetric models.
While not nearly as well-motivated theoretically as supersymmetry, it provides a framework for analyzing anomalies in relatively low activity kinematic regions that superpartner pair-production events cannot easily populate.

In general, while supersymmetry drives the design of many searches for new physics, the signatures of supersymmetry are in practice very general, so that searches nominally targeting supersymmetry are in practice sensitive to a wide variety of models of new physics.
Supersymmetric models are used as a benchmark means of communicating results that is familiar throughout the elementary particle physics community.
Ultimately, the focus of experiments is on providing evidence to support or refute any potential explanations of observations that conflict with the Standard Model prediction.
